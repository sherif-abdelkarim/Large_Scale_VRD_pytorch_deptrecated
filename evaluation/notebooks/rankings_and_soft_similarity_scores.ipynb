{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODOS POST DEADLINE\n",
    "#1. compute the per-class soft-similarities via numpy (to validate current results)\n",
    "#2. long tail analysis with the soft-metrics\n",
    "#3. triplet-based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_csv_file = '/home/optas/DATA/OUT/ltvrd/similarities.csv'\n",
    "similarities_used = ['jcn_similarity', 'word2vec_GNews', 'word2vec_visualVG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(in_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clip similarities to [-1, 1] range.\n",
    "for sim in similarities_used:\n",
    "    too_large = df[sim] > 1    \n",
    "    df.loc[too_large, sim] = 1\n",
    "    too_small = df[sim] < -1\n",
    "    df.loc[too_small, sim] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path  as osp\n",
    "# How can we do the long tail with the soft? \n",
    "# we pick the winner from our table (most likely a hubness) and the baseline.\n",
    "# I want a csv that tells me for every prediction (rel. or sbj.)\n",
    "# a) its class, (optionally the class-frequency), and the top-1 or top-5 [choose]\n",
    "# score per a soft matrix.\n",
    "\n",
    "sim = 'word2vec_GNews' # \n",
    "workon = 'subject' # or relations\n",
    "top_aux_data_dir = '/home/optas/DATA/OUT/ltvrd'\n",
    "\n",
    "freq_info = pd.read_csv(osp.join(top_aux_data_dir, 'gvqa_{}_to_train_freq.csv'.format(workon)))\n",
    "freq_info_dict = dict()\n",
    "\n",
    "if workon == 'subject':\n",
    "    x, y = freq_info.gt_sbj, freq_info.sbj_freq_gt\n",
    "elif workon == 'relations':\n",
    "    x, y = freq_info.gt_rel, freq_info.rel_freq_gt\n",
    "\n",
    "for k, v in zip(x, y):\n",
    "    freq_info_dict[k] = v\n",
    "\n",
    "ndf = df[df['i'] == 0][[sim, 'gold']]\n",
    "g = ndf.groupby('gold')\n",
    "average_per_class = g[sim].mean().reset_index()\n",
    "average_per_class['frequency'] = average_per_class['gold'].apply(lambda x: freq_info_dict[x])\n",
    "average_per_class = average_per_class.sort_values('frequency', ascending=False)\n",
    "average_per_class.to_csv('vgqa_sorted_mean_acc_on_{}_with_{}.csv'.format(workon, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Rank (ignoring misses >250, with pandas)', 1.7125)\n"
     ]
    }
   ],
   "source": [
    "print('Average Rank (ignoring misses >250, with pandas)', df['i'][df['exact_match'] == 1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rank (ignoring misses > 250):  1.734\n",
      "('Total observations:', 79)\n",
      "('Missed in top 250:', 0)\n",
      "Average Rank (with miss penalty 250): 1.734\n",
      "Per Class Average Rank (with miss penalty 250)  2.681\n"
     ]
    }
   ],
   "source": [
    "hits = []\n",
    "hits_per_class = defaultdict(list)\n",
    "totally_missed = 0\n",
    "miss_penalty = 250 # penalty if you didn't find it in the 250-cases\n",
    "\n",
    "for k in range(0, len(df)-250, 250):\n",
    "    \n",
    "    i_th_slice = df.loc[k: k+250-1]\n",
    "    assert i_th_slice['i'].min() == 0\n",
    "    assert i_th_slice['i'].max() == 249    \n",
    "    \n",
    "    hit = np.where(i_th_slice['exact_match'] == 1.0)[0]    \n",
    "    assert len(hit) in [0, 1]\n",
    "    \n",
    "    i_th_class = i_th_slice['gold'].unique()\n",
    "    assert len(i_th_class) == 1\n",
    "    i_th_class = i_th_class[0]\n",
    "    \n",
    "    if len(hit) == 1:\n",
    "        hit = hit[0]\n",
    "        hits.append(hit)\n",
    "        hits_per_class[i_th_class].append(hit)\n",
    "    else:\n",
    "        totally_missed += 1\n",
    "        if miss_penalty > 0:\n",
    "            hits_per_class[i_th_class].append(miss_penalty)\n",
    "        \n",
    "print('Average Rank (ignoring misses > 250):  {:.3f}'.format(np.mean(hits)))\n",
    "print('Total observations:', len(hits))\n",
    "print('Missed in top 250:', totally_missed)\n",
    "penalty_mu = np.mean(hits + [miss_penalty] * totally_missed)\n",
    "print('Average Rank (with miss penalty {}): {:.3f}'.format(miss_penalty, penalty_mu))\n",
    "\n",
    "p_class_mu = []\n",
    "for h in hits_per_class:\n",
    "    p_class_mu.append(np.mean(hits_per_class[h]))    \n",
    "print('Per Class Average Rank (with miss penalty {})  {:.3f}'.format(miss_penalty, np.mean(p_class_mu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class Average Rank (ignore missed): 2.681\n"
     ]
    }
   ],
   "source": [
    "g = df.groupby(['gold'])\n",
    "means_of_groups = []\n",
    "for k, v in g.groups.items():\n",
    "    group_content = df.loc[v]\n",
    "    group_i = group_content[group_content['exact_match'] == 1.0]['i']\n",
    "    if len(group_i) > 0:\n",
    "        group_mean = group_i.mean()\n",
    "        means_of_groups.append(group_mean)\n",
    "    else:\n",
    "        pass # Missed element, ignore.\n",
    "    \n",
    "print ('Per-class Average Rank (ignore missed): {:.3f}'.format(np.mean(means_of_groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average similarity per metric at Top-0', 'jcn_similarity', '0.806')\n",
      "('Average similarity per metric at Top-0', 'word2vec_GNews', '0.845')\n",
      "('Average similarity per metric at Top-0', 'word2vec_visualVG', '0.815')\n"
     ]
    }
   ],
   "source": [
    "for sim in similarities_used:\n",
    "    mu = df[df['i'] == 0][sim].mean()\n",
    "    print ('Average similarity per metric at Top-1', sim, '{:.3f}'.format(mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Per-class similarity per metric at Top-1', 'jcn_similarity', '0.773')\n",
      "('Per-class similarity per metric at Top-1', 'word2vec_GNews', '0.770')\n",
      "('Per-class similarity per metric at Top-1', 'word2vec_visualVG', '0.794')\n"
     ]
    }
   ],
   "source": [
    "for sim in similarities_used:    \n",
    "    for k in [1]:\n",
    "        ndf = df[df['i'] < k]\n",
    "        max_per_image_gold = ndf.groupby(['image_id', 'gold'])[sim].max() # Missed Corner-case (bag/bag in same image.)\n",
    "        mu = max_per_image_gold.groupby('gold').mean().mean()\n",
    "        print ('Per-class similarity per metric at Top-{}'.format(k), sim, '{:.3f}'.format(mu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27-joint-DL",
   "language": "python",
   "name": "py27jointdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
